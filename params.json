{"name":"Celery-S3","tagline":"An S3 result backend for Celery.","body":"# Welcome\r\n\r\nCelery-S3 is a simple S3 result backend for Celery.\r\n\r\nIf used in conjunction with the SQS broker, it allows for Celery deployments\r\nthat use only distributed AWS services -- with no dependency on individual\r\nmachines within your infrastructure.\r\n\r\nThis backend probably isn't suitable for particularly high-traffic Celery\r\ndeployments, but it works just fine in general -- and imposes no limits on the\r\nnumber of workers in the pool.\r\n\r\n## Installation\r\n\r\nInstall via pip:\r\n\r\n`pip install celery-s3`\r\n\r\nThen configure Celery to use the `S3Backend`:\r\n\r\n    CELERY_RESULT_BACKEND = 'celery_s3.backends.S3Backend'\r\n\r\n    CELERY_S3_BACKEND_SETTINGS = {\r\n        'aws_access_key_id': '<your_aws_access_key_id>',\r\n        'aws_secret_access_key': '<your_aws_secret_access_key>',\r\n        'bucket': '<your_bucket_name>',\r\n    }\r\n\r\n## Configuration\r\n\r\nTo use a folder within the specified bucket, set the `base_path` in your\r\n`CELERY_S3_BACKEND_SETTINGS`:\r\n\r\n\r\n    CELERY_S3_BACKEND_SETTINGS = {\r\n        ...\r\n        'base_path': '/celery/',\r\n        ...\r\n    }\r\n\r\n## Notes\r\n\r\nStoring Celery results with this backend will obviously result in API calls\r\nbeing made to Amazon S3.  For each result, at least one `PUT` request will be\r\nmade (priced at $0.01 per 1,000 requests at the time of writing).  Also, the\r\ndata contained within the result object will be stored indefinitely, unless\r\notherwise specified.\r\n\r\nTo fetch a result for a task that has already finished, at least two requests\r\nwill be made (one `HEAD` and one `GET`).  If you use Celery's `result.get()` to\r\nwait for a task to finish, S3 will be polled continuously until the task has\r\nfinished.\r\n\r\nBy default, the poll interval is set to 0.5 seconds, which could result in\r\na lot of requests (two `HEAD` requests per second until the task has finished,\r\nthen one `GET` request to fetch the result).  If you need to use\r\n`result.get()`, consider increasing the interval and using a timeout to prevent\r\npolling forever: `result.get(interval=5, timeout=600)`.\r\n\r\nAlso, for tasks whose result you don't need, be sure to use `ignore_result`:\r\n\r\n    @celery.task(ignore_result=True)\r\n    def process_data(obj):\r\n        obj.do_processing()\r\n\r\nOnce task results have been used and are no longer needed, be sure to call\r\n`result.forget()` to delete the corresponding S3 key.  Otherwise, old results\r\nwill remain forever and contribute to storage costs (storage is priced at\r\n$0.095 per GB per month at the time of writing).\r\n\r\nAlso, the S3 lifecycle can be used to archive or delete old keys after\r\na certain period of time.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}